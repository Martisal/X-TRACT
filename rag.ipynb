{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import uuid\n",
    "from transformers import AutoTokenizer,AutoModel\n",
    "import torch\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers.pipelines import pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'sentence-transformers/all-mpnet-base-v2' #\"BAAI/bge-small-en-v1.5\"\n",
    "datout = 'refertijson'\n",
    "\n",
    "jsonkeys = ['AAID', \n",
    "            'indicazioni al test', \n",
    "            'risultato',\n",
    "            'inviante',\n",
    "            'materiale','materiale inviato','materiale ricevuto',\n",
    "            'test','test eseguito','test richiesto',\n",
    "            'dettagli',\n",
    "            'interpretazione',\n",
    "            'suggerimenti',\n",
    "            'metodo',\n",
    "            'interpretazione',\n",
    "            'limiti']\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name, device_map=\"cuda\")\n",
    "\n",
    "fnames = os.listdir(datout)\n",
    "jdata = []\n",
    "reports = []\n",
    "\n",
    "for fname in fnames:\n",
    "    with open(os.path.join(datout,fname),'r') as f:\n",
    "        jf = json.load(f)\n",
    "        jdata.append(jf)\n",
    "        \n",
    "for j in jdata:\n",
    "    report = ''\n",
    "    for jk in jsonkeys: \n",
    "        if jk in j.keys():\n",
    "            report += jk + ': ' + j[jk] + '\\n#####\\n'\n",
    "\n",
    "    if not 'risultato' in j.keys():\n",
    "        report += 'rescode: esito sconosciuto'        \n",
    "    elif j['risultato'].startswith('non'):\n",
    "        report += 'rescode: caso negativo'\n",
    "    else:\n",
    "        report += 'rescode: caso positivo'    \n",
    "        \n",
    "    reports.append(report)\n",
    "    \n",
    "\n",
    "print('Dataset length:',len(jdata))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunk the text so as to optimize it for retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_chunker(reports,\n",
    "                     model_name,\n",
    "                     paragraph_separator='\\n#####\\n',\n",
    "                     chunk_size=1024,\n",
    "                     separator=' ',\n",
    "                     secondary_chunking_regex=r'\\S+?[\\.,;!?]',\n",
    "                     chunk_overlap=0):\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)  # Load tokenizer for the specified model\n",
    "    documents = {}  # Initialize dictionary to store results\n",
    "\n",
    "\n",
    "    for text in reports:\n",
    "\n",
    "        aaid = text[6:text.index('.txt')]\n",
    "        \n",
    "        paragraphs = re.split(paragraph_separator, text)[1:]\n",
    "        all_chunks = {}\n",
    "        for paragraph in paragraphs:\n",
    "            field = paragraph.split(':')[0]\n",
    "            words = paragraph.split(separator)\n",
    "            current_chunk = \"\"\n",
    "            chunks = []\n",
    "\n",
    "            for word in words:\n",
    "                new_chunk = current_chunk + (separator if current_chunk else '') + word\n",
    "                if len(tokenizer.tokenize(new_chunk)) <= chunk_size:\n",
    "                    current_chunk = new_chunk\n",
    "                else:\n",
    "                    if current_chunk:\n",
    "                        chunks.append(current_chunk)\n",
    "                    current_chunk = word\n",
    "\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "\n",
    "            refined_chunks = []\n",
    "            for chunk in chunks:\n",
    "                if len(tokenizer.tokenize(chunk)) > chunk_size:\n",
    "                    sub_chunks = re.split(secondary_chunking_regex, chunk)\n",
    "                    sub_chunk_accum = \"\"\n",
    "                    for sub_chunk in sub_chunks:\n",
    "                        if sub_chunk_accum and len(tokenizer.tokenize(sub_chunk_accum + sub_chunk + ' ')) > chunk_size:\n",
    "                            refined_chunks.append(sub_chunk_accum.strip())\n",
    "                            sub_chunk_accum = sub_chunk\n",
    "                        else:\n",
    "                            sub_chunk_accum += (sub_chunk + ' ')\n",
    "                    if sub_chunk_accum:\n",
    "                        refined_chunks.append(sub_chunk_accum.strip())\n",
    "                else:\n",
    "                    refined_chunks.append(chunk)\n",
    "\n",
    "            final_chunks = []\n",
    "            if chunk_overlap > 0 and len(refined_chunks) > 1:\n",
    "                for i in range(len(refined_chunks) - 1):\n",
    "                    final_chunks.append(refined_chunks[i])\n",
    "                    overlap_start = max(0, len(refined_chunks[i]) - chunk_overlap)\n",
    "                    overlap_end = min(chunk_overlap, len(refined_chunks[i+1]))\n",
    "                    overlap_chunk = refined_chunks[i][overlap_start:] + ' ' + refined_chunks[i+1][:overlap_end]\n",
    "                    final_chunks.append(overlap_chunk)\n",
    "                final_chunks.append(refined_chunks[-1])\n",
    "            else:\n",
    "                final_chunks = refined_chunks\n",
    "\n",
    "            # Assign a UUID for each chunk and structure it with text and metadata\n",
    "            for chunk in final_chunks:\n",
    "                chunk_id = str(uuid.uuid4())\n",
    "                all_chunks[chunk_id] = {\"text\": chunk, \"metadata\": {\"file_name\":aaid, \"field\":field}}  # Initialize metadata as dict\n",
    "\n",
    "        # Map the document UUID to its chunk dictionary\n",
    "        documents[aaid] = all_chunks\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('ragembeddings/docs-v2'):\n",
    "    with open('ragembeddings/docs-v2','br') as f:\n",
    "        docs = pickle.load(f)\n",
    "else:\n",
    "    print('Document chunks not found. Computing them (may take several minutes)...')        \n",
    "    docs = document_chunker(reports=reports,\n",
    "                        model_name=model_name,\n",
    "                        chunk_size=256)\n",
    "    with open('ragembeddings/docs-v2','bw') as f:\n",
    "        pickle.dump(docs,f)\n",
    "\n",
    "keys = list(docs.keys())\n",
    "print(len(docs))\n",
    "print(keys)\n",
    "print(docs[keys[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(text):\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to('cuda') \n",
    "    \n",
    "    # Generate the embeddings \n",
    "    with torch.no_grad():    \n",
    "        embeddings = model(**inputs).last_hidden_state.mean(dim=1).squeeze()\n",
    "\n",
    "    return embeddings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(doc_store):\n",
    "    vector_store = {}\n",
    "    for doc_id, chunks in doc_store.items():\n",
    "        doc_vectors = {}\n",
    "        for chunk_id, chunk_dict in chunks.items():\n",
    "            # Generate an embedding for each chunk of text\n",
    "            doc_vectors[chunk_id] = compute_embeddings(chunk_dict.get(\"text\"))\n",
    "        # Store the document's chunk embeddings mapped by their chunk UUIDs\n",
    "        vector_store[doc_id] = doc_vectors\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_matches(vector_store, query_str, top_k):\n",
    "    \"\"\"\n",
    "    This function takes in a vector store dictionary, a query string, and an int 'top_k'.\n",
    "    It computes embeddings for the query string and then calculates the cosine similarity against every chunk embedding in the dictionary.\n",
    "    The top_k matches are returned based on the highest similarity scores.\n",
    "    \"\"\"\n",
    "    # Get the embedding for the query string\n",
    "    query_str_embedding = np.array(compute_embeddings(query_str))\n",
    "    scores = {}\n",
    "\n",
    "    # Calculate the cosine similarity between the query embedding and each chunk's embedding\n",
    "    for doc_id, chunks in vector_store.items():\n",
    "        for chunk_id, chunk_embedding in chunks.items():\n",
    "            chunk_embedding_array = np.array(chunk_embedding)\n",
    "            # Normalize embeddings to unit vectors for cosine similarity calculation\n",
    "            norm_query = np.linalg.norm(query_str_embedding)\n",
    "            norm_chunk = np.linalg.norm(chunk_embedding_array)\n",
    "            if norm_query == 0 or norm_chunk == 0:\n",
    "                # Avoid division by zero\n",
    "                score = 0\n",
    "            else:\n",
    "                score = np.dot(chunk_embedding_array, query_str_embedding) / (norm_query * norm_chunk)\n",
    "\n",
    "            # Store the score along with a reference to both the document and the chunk\n",
    "            scores[(doc_id, chunk_id)] = score\n",
    "\n",
    "    # Sort scores and return the top_k results\n",
    "    sorted_scores = sorted(scores.items(), key=lambda item: item[1], reverse=True)[:top_k]\n",
    "    top_results = [(doc_id, chunk_id, score) for ((doc_id, chunk_id), score) in sorted_scores]\n",
    "\n",
    "    return top_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_doc(vector_store, docs, query_str, threshold=0.7):\n",
    "    \"\"\"\n",
    "    This function takes in a vector store dictionary, a query string, and similarity threshold.\n",
    "    It computes embeddings for each atom from the query string and then calculates the cosine similarity against every chunk embedding in the dictionary.\n",
    "    If a document appear for each atom with a similarity higher than the threshold, then the document is returned.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    alltop = []\n",
    "    for atom in query_str.split('AND'):\n",
    "        scores = {}\n",
    "\n",
    "        query_str_embedding = np.array(compute_embeddings(atom))\n",
    "        for doc_id, chunks in vector_store.items():\n",
    "                \n",
    "            for chunk_id, chunk_embedding in chunks.items():\n",
    "                \n",
    "                #inviante\n",
    "                if 'dr.' in atom.lower() or 'dott.' in atom.lower() or 'dottor' in atom.lower() or 'invia' in atom.lower():\n",
    "                    if docs[doc_id][chunk_id]['text'].startswith('inviante') and atom.split()[-1] in docs[doc_id][chunk_id]['text']:\n",
    "                        scores[(doc_id, chunk_id)] = 1\n",
    "                        continue \n",
    "\n",
    "                #variante   \n",
    "                if 'variante' in atom.lower() and 'p.' in atom:\n",
    "                    if atom[atom.index('p.')+2:atom.index('p.')+10].lower() in docs[doc_id][chunk_id]['text']:\n",
    "                        scores[(doc_id, chunk_id)] = 1\n",
    "                        continue\n",
    "                \n",
    "                #gene  \n",
    "                if 'gene' in atom.lower():\n",
    "                    if len(atom.split()) > atom.split().index('gene') and atom[atom.split().index('gene')+1] in docs[doc_id][chunk_id]['text']: \n",
    "                        scores[(doc_id, chunk_id)] = 1\n",
    "                        continue\n",
    "                          \n",
    "\n",
    "                chunk_embedding_array = np.array(chunk_embedding)\n",
    "                # Normalize embeddings to unit vectors for cosine similarity calculation\n",
    "                norm_query = np.linalg.norm(query_str_embedding)\n",
    "                norm_chunk = np.linalg.norm(chunk_embedding_array)\n",
    "                if norm_query == 0 or norm_chunk == 0:\n",
    "                    # Avoid division by zero\n",
    "                    score = 0\n",
    "                else:\n",
    "                    score = np.dot(chunk_embedding_array, query_str_embedding) / (norm_query * norm_chunk)\n",
    "\n",
    "                # Store the score along with a reference to both the document and the chunk\n",
    "                scores[(doc_id, chunk_id)] = score\n",
    "\n",
    "        sorted_scores = sorted(scores.items(), key=lambda item: item[1], reverse=True)\n",
    "        top_results = [(doc_id, chunk_id, score) for ((doc_id, chunk_id), score) in sorted_scores if score > threshold]\n",
    "        print(len(sorted_scores), len(top_results))\n",
    "        alltop.append(top_results)\n",
    "    \n",
    "    if len(alltop) > 1:\n",
    "        retrieved = []\n",
    "        for at in alltop[0]:\n",
    "            curchunks = [at]\n",
    "            for i in range(1,len(alltop)):\n",
    "                found = False\n",
    "                j = 0\n",
    "                while j < len(alltop[i]) and found == False:\n",
    "                    if alltop[i][j][0] == at[0]:\n",
    "                        found = True\n",
    "                        curchunks.append(alltop[i][j])\n",
    "                    j += 1\n",
    "                if found:\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "            if found:\n",
    "                retrieved += curchunks            \n",
    "\n",
    "    else: \n",
    "        retrieved = alltop[0]    \n",
    "\n",
    "    return retrieved    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load vector store, if exists. Otherwise create and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('ragembeddings/mpnet-v2'):\n",
    "    print('Vector store for this model already exists. Loading vectors.')\n",
    "    with open('ragembeddings/mpnet-v2','br') as f:\n",
    "        vec_store = pickle.load(f)\n",
    "else:        \n",
    "    print('Vector store for this model doesn\\'t exists. Creating vectors (may take long time)...')\n",
    "    vec_store = create_vector_store(docs)\n",
    "    with open('ragembeddings/mpnet-v2','bw') as f:\n",
    "        pickle.dump(vec_store,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matches = compute_matches(vector_store=vec_store,query_str=\"casi con una mutazione nel gene SMARCB1\",top_k=10)\n",
    "matches = retrieve_doc(vector_store=vec_store,\n",
    "                       docs=docs,\n",
    "                       query_str=\"test intero esoma\", #\"variante p.His269Arg\",\n",
    "                       threshold=0.7)\n",
    "\n",
    "\n",
    "for rep in matches:   \n",
    "    print(rep)\n",
    "    #print(docs[rep[0]])\n",
    "    print(docs[rep[0]][rep[1]])\n",
    "    print('-----')\n",
    "    \n",
    "doc_ids = set([m[0] for m in matches])\n",
    "for d in doc_ids:\n",
    "    print(d, docs[d])\n",
    "\n",
    "print(len(doc_ids), 'retrieved documents')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimental setup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
